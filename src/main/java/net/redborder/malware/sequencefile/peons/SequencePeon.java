package net.redborder.malware.sequencefile.peons;

import com.amazonaws.util.IOUtils;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.yaml.YAMLFactory;
import com.jcraft.jsch.Channel;
import com.jcraft.jsch.ChannelExec;
import com.jcraft.jsch.JSch;
import com.jcraft.jsch.JSchException;
import com.jcraft.jsch.Session;

import net.redborder.malware.sequencefile.SequenceFile;
import net.redborder.malware.sequencefile.util.ConfigFile;
import net.redborder.malware.sequencefile.util.S3HDFSDB;
import org.apache.commons.codec.digest.DigestUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.log4j.Logger;
import org.jets3t.service.S3Service;
import org.jets3t.service.ServiceException;
import org.jets3t.service.impl.rest.httpclient.RestS3Service;
import org.jets3t.service.model.S3Object;
import org.jets3t.service.security.AWSCredentials;

import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.UUID;
import java.util.concurrent.TimeUnit;

/**
 * Created by andresgomez on 3/2/15.
 */
public class SequencePeon extends Thread {
    private static final Logger log = Logger.getLogger(SequencePeon.class);
    S3Service s3Service;
    SequenceFile seq;
    String hdfsURL;
    private S3HDFSDB database;
    private String bucket;
    private String mode;
    private List<String> logstashManagers;
    private String LOGSTASH_PATH;
    
    public SequencePeon(SequenceFile seq) {
        this.seq = seq;
        mode = (String) ConfigFile.getInstance().getFromGeneral("mode","hadoop");
        AWSCredentials awsCredentials = new AWSCredentials((String) ConfigFile.getInstance().getFromGeneral("aws_access",""), (String) ConfigFile.getInstance().getFromGeneral("aws_secret",""));
        bucket = ConfigFile.getInstance().getFromGeneral("aws_bucket","");
        s3Service = new RestS3Service(awsCredentials);
        database = S3HDFSDB.getInstance();
    }
    
    public SequencePeon(SequenceFile seq, String LOGSTASH_PATH) {
        this(seq);
        this.LOGSTASH_PATH = LOGSTASH_PATH;
        logstashManagers = getLogstashManagers();
    }
    
    /**
     * 
     * @return
     */
    private List<String> getLogstashManagers() {

    		ObjectMapper mapper = new ObjectMapper(new YAMLFactory());
    		Map<String, Object> data;
    		List<String> logstashManagers = new ArrayList<String>();
    		try {
    			data = mapper.readValue(new File("/opt/rb/etc/managers.yml"), Map.class);
    		
    			logstashManagers = (List<String>) data.get("logstash");

    		} catch (IOException e) {
    			
    			e.printStackTrace();
    		}
    		
    		return logstashManagers;
    }
    
    private void scp(byte[] bytes, String host) {
    	ByteArrayInputStream fis=null;
	    try{
	      
	      String user="root";
	      String task = UUID.randomUUID().toString();
	      
	      String rfile= LOGSTASH_PATH + task;

	      JSch jsch=new JSch();
	      jsch.setKnownHosts("/root/.ssh/known_hosts"); 	
	      jsch.addIdentity("/opt/rb/var/www/rb-rails/config/rsa"); 	
	      
	      Session session=jsch.getSession(user, host, 22);
	     
	      session.connect();
	      
	      //boolean ptimestamp = true;

	      // exec 'scp -t rfile' remotely
	      rfile=rfile.replace("'", "'\"'\"'");
	      rfile="'"+rfile+"'";
	      //String command="scp " + (ptimestamp ? "-p" :"") +" -t "+rfile;
	      String command="scp " + " -t " + rfile;
	      Channel channel=session.openChannel("exec");
	      ((ChannelExec)channel).setCommand(command);

	      // get I/O streams for remote scp
	      OutputStream out=channel.getOutputStream();
	      InputStream in=channel.getInputStream();

	      channel.connect();

	      if(checkAck(in)!=0){System.exit(0);}

	      // send "C0644 filesize filename", where filename should not include '/'
	      long filesize=bytes.length;
	      command="C0644 " + filesize + " " + task + "\n";
	      //command+=bytes + "\n";
	      
	      out.write(command.getBytes()); out.flush();
	      if(checkAck(in)!=0){System.exit(0);}
	      
	      // send a content of lfile
	      fis=new ByteArrayInputStream(bytes);
	      byte[] buf=new byte[1024];
	      
	      while(true){
	    	int len=fis.read(buf, 0, buf.length);
	    	if(len<=0) break;
	        out.write(buf, 0, len); out.flush();
	      }

	      fis.close();
	      fis=null;

	      buf[0]=0; out.write(buf, 0, 1); out.flush();
	      if(checkAck(in)!=0){System.exit(0);}
	      
	      out.close();

	      channel.disconnect();
	      session.disconnect();

	    }
	    catch(JSchException | IOException e){
	      log.info(e);
	      log.info(e.getCause());
	      try{if(fis!=null)fis.close();}catch(Exception ee){}
	    }
	  }
    
    private static int checkAck(InputStream in) throws IOException{
	    int b=in.read();
	    // b may be 0 for success,
	    //          1 for error,
	    //          2 for fatal error,
	    //          -1
	    if(b==0 || b==1) return b;

	    if(b==1 || b==2){
	      StringBuffer sb=new StringBuffer();
	      int c;
	      do {
		c=in.read();
		sb.append((char)c);
	      }
	      while(c!='\n');
	      if(b==1){ // error
		System.out.print(sb.toString());
	      }
	      if(b==2){ // fatal error
		System.out.print(sb.toString());
	      }
	    }
	    return b;
	  }
	  
    private void scpS3Object(S3Object s3Object) {
        byte[] bytes;
        
		try {
			bytes = IOUtils.toByteArray(s3Object.getDataInputStream());
			
			Random rand = new Random();
		    String manager = logstashManagers.get(rand.nextInt(logstashManagers.size()));
		    scp(bytes,manager);
		    
		 }catch (IOException | ServiceException e) {
			 log.info(e.getMessage());
			 }
    }

    @Override
    public void run() {

        log.info("Launching sequence peon [" + Thread.currentThread().getId() + "]");
        log.info("Mode [" + mode + "]");
        
        org.apache.hadoop.io.SequenceFile.Writer writer = null;
        
        if (mode.equals("hadoop")){
	        Configuration conf = new Configuration();
	        conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
	        conf.addResource(new Path("/opt/rb/var/hadoop/etc/hadoop/core-site.xml"));
	        conf.addResource(new Path("/opt/rb/var/hadoop/etc/hadoop/hdfs-site.xml"));
	
	        Path name = new Path(seq.getHDFSPath());
	        
	
	        boolean retry = false;
	
	        do {
	
	            try {
	
	                if(retry) {
	                    TimeUnit.SECONDS.sleep(15);
	                    retry = false;
	                }
	
	                org.apache.hadoop.io.SequenceFile.Writer.Option filePath = org.apache.hadoop.io.SequenceFile.Writer.file(name);
	                org.apache.hadoop.io.SequenceFile.Writer.Option keyClass = org.apache.hadoop.io.SequenceFile.Writer.keyClass(Text.class);
	                org.apache.hadoop.io.SequenceFile.Writer.Option valueClass = org.apache.hadoop.io.SequenceFile.Writer.valueClass(BytesWritable.class);
	                writer = org.apache.hadoop.io.SequenceFile.createWriter(conf, filePath, keyClass, valueClass);
	
	            } catch (IOException e) {
	                log.debug(e.getMessage());
	                log.info("Failed writing to HDFS. Sleep 15 seconds and retry!");
	                retry = true;
	            } catch (InterruptedException e) {
	                log.debug(e.getMessage());
	                log.info("Thread interrupted. Sleep 15 seconds and retry!");
	            }
	
	        }while (retry);
	        
        }
	
        Text key = new Text();
        BytesWritable val = new BytesWritable();
        long totalBytes = 0L;
        List<String> files = seq.getS3FilesPaths();

        for (String file : files) {
            S3Object s3Object;
            log.info("Sending file "+ file +" from s3 to " + StringUtils.capitalize(mode));
            try {
                s3Object = s3Service.getObject(bucket, file);
                
                if (mode.equals("hadoop")){
	                byte[] bytes = IOUtils.toByteArray(s3Object.getDataInputStream());
	                totalBytes += bytes.length;
	                val.set(bytes, 0, bytes.length);
	                key.set(DigestUtils.sha256Hex(bytes));
	                writer.append(key, val);
	                
                }else if(mode.equals("logstash")){
                	scpS3Object(s3Object);
                }
                
                s3Object.closeDataInputStream();

            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        try {
            if (mode.equals("hadoop")) {writer.close();}
        } catch (IOException e) {
            e.printStackTrace();
        }

        if (mode.equals("hadoop")){
	        log.info("Wrote sequence file [ " + totalBytes + "B ] at: " + seq.getHDFSPath());
	        hdfsURL = seq.getHDFSPath();
        }
    }


    public String getHdfsFile() {
        return hdfsURL;
    }
}

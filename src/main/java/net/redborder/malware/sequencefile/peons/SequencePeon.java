package net.redborder.malware.sequencefile.peons;

import com.amazonaws.util.IOUtils;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.yaml.YAMLFactory;

import net.redborder.malware.sequencefile.SequenceFile;
import net.redborder.malware.sequencefile.util.ConfigFile;
import net.redborder.malware.sequencefile.util.S3HDFSDB;
import org.apache.commons.codec.digest.DigestUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.log4j.Logger;
import org.jets3t.service.S3Service;
import org.jets3t.service.ServiceException;
import org.jets3t.service.impl.rest.httpclient.RestS3Service;
import org.jets3t.service.model.S3Object;
import org.jets3t.service.security.AWSCredentials;

import java.io.BufferedReader;
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.FileWriter; 
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.UUID;
import java.util.concurrent.TimeUnit;

/**
 * Created by andresgomez on 3/2/15.
 */
public class SequencePeon extends Thread {
    private static final Logger log = Logger.getLogger(SequencePeon.class);
    S3Service s3Service;
    SequenceFile seq;
    String hdfsURL;
    private S3HDFSDB database;
    private String bucket;
    private String mode;
    private String LOGSTASH_PATH;
	private String rsaKey;
	private List<String> logstashManagers;
    
    public SequencePeon(SequenceFile seq) {
        this.seq = seq;
        mode = (String) ConfigFile.getInstance().getFromGeneral("mode","hadoop");
		logstashManagers = ConfigFile.getInstance().getLogstashManagers();
		rsaKey= ConfigFile.getInstance().getFromGeneral("rsa_key","");

        AWSCredentials awsCredentials = new AWSCredentials((String) ConfigFile.getInstance().getFromGeneral("aws_access",""), (String) ConfigFile.getInstance().getFromGeneral("aws_secret",""));
        bucket = ConfigFile.getInstance().getFromGeneral("aws_bucket","");
        s3Service = new RestS3Service(awsCredentials);
        database = S3HDFSDB.getInstance();
    }
    
    public SequencePeon(SequenceFile seq, String LOGSTASH_PATH) {
        this(seq);
        this.LOGSTASH_PATH = LOGSTASH_PATH;
    }
    
    /**
     * 
     * @return
     */
    private String getRandomLogstashManagers() {   		
			Random rand = new Random();
			String manager = logstashManagers.get(rand.nextInt(logstashManagers.size()));
    		return manager;
    }
    
    private void scp(String source, String destiny, String host, String user, String key) {
    	try {
			
			log.info("Moving file("+source+") to ("+destiny+") in the manager "+host+".");

    		Process proc = Runtime.getRuntime().exec(new String[]{	"/usr/bin/rsync", "-aqvz", "-e", 
    																"ssh -o ConnectTimeout=5 -o LogLevel=quiet "
    															  + "-o UserKnownHostsFile=/dev/null "
    															  + "-o PasswordAuthentication=no "
    															  + "-o StrictHostKeyChecking=no "
    															  + "-i "+key, 
    															  	source,
																	user +"@" + host + ":" + destiny});
    		//proc.wait(15000);
    		proc.waitFor();

			log.info("Exit value: " + proc.exitValue());

            BufferedReader stdError = new BufferedReader(new InputStreamReader(proc.getErrorStream()));
  
            String s = null;
            
        	while ((s = stdError.readLine()) != null) {
        	   log.error(s);
            }
    		
        } catch (IOException e) {
        	log.error(e.getMessage());
        } catch (InterruptedException e) {
        	log.error("Timeout trying to send files to s3.");
		}
	  }
	  
    @Override
    public void run() {

        log.info("Launching sequence peon [" + Thread.currentThread().getId() + "]");
        log.info("Mode [" + mode + "]");
        
        org.apache.hadoop.io.SequenceFile.Writer writer = null;
        
        if (mode.equals("hadoop")){
	        Configuration conf = new Configuration();
	        conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
	        conf.addResource(new Path("/opt/rb/var/hadoop/etc/hadoop/core-site.xml"));
	        conf.addResource(new Path("/opt/rb/var/hadoop/etc/hadoop/hdfs-site.xml"));
	
	        Path name = new Path(seq.getHDFSPath());
	        
	
	        boolean retry = false;
	
	        do {
	
	            try {
	
	                if(retry) {
	                    TimeUnit.SECONDS.sleep(15);
	                    retry = false;
	                }
	
	                org.apache.hadoop.io.SequenceFile.Writer.Option filePath = org.apache.hadoop.io.SequenceFile.Writer.file(name);
	                org.apache.hadoop.io.SequenceFile.Writer.Option keyClass = org.apache.hadoop.io.SequenceFile.Writer.keyClass(Text.class);
	                org.apache.hadoop.io.SequenceFile.Writer.Option valueClass = org.apache.hadoop.io.SequenceFile.Writer.valueClass(BytesWritable.class);
	                writer = org.apache.hadoop.io.SequenceFile.createWriter(conf, filePath, keyClass, valueClass);
	
	            } catch (IOException e) {
	                log.debug(e.getMessage());
	                log.info("Failed writing to HDFS. Sleep 15 seconds and retry!");
	                retry = true;
	            } catch (InterruptedException e) {
	                log.debug(e.getMessage());
	                log.info("Thread interrupted. Sleep 15 seconds and retry!");
	            }
	
	        }while (retry);
	        
        }
	
        Text key = new Text();
        BytesWritable val = new BytesWritable();
        long totalBytes = 0L;
        List<String> files = seq.getS3FilesPaths();

        for (String file : files) {
            S3Object s3Object;
            log.info("Sending file "+ file +" from s3 to " + StringUtils.capitalize(mode));
            try {
                s3Object = s3Service.getObject(bucket, file);
                byte[] bytes = IOUtils.toByteArray(s3Object.getDataInputStream());
				totalBytes += bytes.length;

                if (mode.equals("hadoop")){
	                val.set(bytes, 0, bytes.length);
	                key.set(DigestUtils.sha256Hex(bytes));
	                writer.append(key, val);
	                
                }else if(mode.equals("logstash")){
					try {
						String tmpDirectory ="/tmp/";
						String fileName = UUID.randomUUID().toString();
						String filePath = tmpDirectory+fileName;
						String fileLockName = fileName + ".lock";
						String fileLockPath = tmpDirectory+fileLockName;
	
						String destinyFilePath = LOGSTASH_PATH + fileName;
						String destinyLockPath = LOGSTASH_PATH + fileLockName;
						
						// Create file
						java.nio.file.Path path = Paths.get(filePath);
						Files.write(path, bytes);
						
						// Create lock file	it will contain the destiny path e.g. /usr/share/logstash/malware/<randomUUID>
						FileWriter lockFile = new FileWriter(fileLockPath);	
						lockFile.write(destinyFilePath);
						lockFile.close();

						String manager = getRandomLogstashManagers();
						
						scp(filePath, destinyFilePath, manager, "root", rsaKey);
						scp(fileLockPath, destinyLockPath, manager, "root", rsaKey);

						// Clean files
						File f = new File(filePath);
						f.delete();
						f = new File(fileLockPath);
						f.delete();
					} catch (IOException e) {
						log.error(e.getMessage());
					}
                }
                
                s3Object.closeDataInputStream();

            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        try {
            if (mode.equals("hadoop")) {writer.close();}
        } catch (IOException e) {
            e.printStackTrace();
        }

        if (mode.equals("hadoop")){
	        log.info("Wrote sequence file [ " + totalBytes + "B ] at: " + seq.getHDFSPath());
	        hdfsURL = seq.getHDFSPath();
        }
    }


    public String getHdfsFile() {
        return hdfsURL;
    }
}

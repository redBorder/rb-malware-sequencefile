package net.redborder.malware.sequencefile.peons;

import com.amazonaws.util.IOUtils;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.dataformat.yaml.YAMLFactory;

import net.redborder.malware.sequencefile.SequenceFile;
import net.redborder.malware.sequencefile.util.ConfigFile;
import net.redborder.malware.sequencefile.util.S3HDFSDB;
import org.apache.commons.codec.digest.DigestUtils;
import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.log4j.Logger;
import org.jets3t.service.S3Service;
import org.jets3t.service.ServiceException;
import org.jets3t.service.impl.rest.httpclient.RestS3Service;
import org.jets3t.service.model.S3Object;
import org.jets3t.service.security.AWSCredentials;

import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.UUID;
import java.util.concurrent.TimeUnit;

/**
 * Created by andresgomez on 3/2/15.
 */
public class SequencePeon extends Thread {
    private static final Logger log = Logger.getLogger(SequencePeon.class);
    S3Service s3Service;
    SequenceFile seq;
    String hdfsURL;
    private S3HDFSDB database;
    private String bucket;
    private String mode;
    private List<String> logstashManagers;
    private String LOGSTASH_PATH;
    
    public SequencePeon(SequenceFile seq) {
        this.seq = seq;
        mode = (String) ConfigFile.getInstance().getFromGeneral("mode","hadoop");
        AWSCredentials awsCredentials = new AWSCredentials((String) ConfigFile.getInstance().getFromGeneral("aws_access",""), (String) ConfigFile.getInstance().getFromGeneral("aws_secret",""));
        bucket = ConfigFile.getInstance().getFromGeneral("aws_bucket","");
        s3Service = new RestS3Service(awsCredentials);
        database = S3HDFSDB.getInstance();
    }
    
    public SequencePeon(SequenceFile seq, String LOGSTASH_PATH) {
        this(seq);
        this.LOGSTASH_PATH = LOGSTASH_PATH;
        logstashManagers = getLogstashManagers();
    }
    
    /**
     * 
     * @return
     */
    private List<String> getLogstashManagers() {

    		ObjectMapper mapper = new ObjectMapper(new YAMLFactory());
    		Map<String, Object> data;
    		List<String> logstashManagers = new ArrayList<String>();
    		try {
    			data = mapper.readValue(new File("/opt/rb/etc/managers.yml"), Map.class);
    		
    			logstashManagers = (List<String>) data.get("logstash");

    		} catch (IOException e) {
    			
    			e.printStackTrace();
    		}
    		
    		return logstashManagers;
    }
    
    private void scp(byte[] bytes, String host) {
    	String task = UUID.randomUUID().toString();
    	Path path = Paths.get("/tmp/" + task);
    	
    	Files.write(path, bytes);
    	
    	String dest_path = LOGSTASH_PATH + task;

    	String command = "/usr/bin/rsync -aqvz -e ssh -o ConnectTimeout=5 -o LogLevel=quiet "
    					+ "-o UserKnownHostsFile=/dev/null -o PasswordAuthentication=no "
    					+ "-o StrictHostKeyChecking=no -i /opt/rb/var/www/rb-rails/config/rsa "
    					+ "/tmp/" + task + " root@" + host + ":" + dest_path;
    	
    	try {
            process = Runtime.getRuntime().exec(command);
            File file = new File("/tmp/" + task);
            file.delete()
        } catch (IOException e) {
            e.printStackTrace();
        }
	  }
	  
    private void scpS3Object(S3Object s3Object) {
        byte[] bytes;
        
		try {
			bytes = IOUtils.toByteArray(s3Object.getDataInputStream());
			
			Random rand = new Random();
		    String manager = logstashManagers.get(rand.nextInt(logstashManagers.size()));
		    scp(bytes,manager);
		    
		 }catch (IOException | ServiceException e) {
			 log.info(e.getMessage());
			 }
    }

    @Override
    public void run() {

        log.info("Launching sequence peon [" + Thread.currentThread().getId() + "]");
        log.info("Mode [" + mode + "]");
        
        org.apache.hadoop.io.SequenceFile.Writer writer = null;
        
        if (mode.equals("hadoop")){
	        Configuration conf = new Configuration();
	        conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
	        conf.addResource(new Path("/opt/rb/var/hadoop/etc/hadoop/core-site.xml"));
	        conf.addResource(new Path("/opt/rb/var/hadoop/etc/hadoop/hdfs-site.xml"));
	
	        Path name = new Path(seq.getHDFSPath());
	        
	
	        boolean retry = false;
	
	        do {
	
	            try {
	
	                if(retry) {
	                    TimeUnit.SECONDS.sleep(15);
	                    retry = false;
	                }
	
	                org.apache.hadoop.io.SequenceFile.Writer.Option filePath = org.apache.hadoop.io.SequenceFile.Writer.file(name);
	                org.apache.hadoop.io.SequenceFile.Writer.Option keyClass = org.apache.hadoop.io.SequenceFile.Writer.keyClass(Text.class);
	                org.apache.hadoop.io.SequenceFile.Writer.Option valueClass = org.apache.hadoop.io.SequenceFile.Writer.valueClass(BytesWritable.class);
	                writer = org.apache.hadoop.io.SequenceFile.createWriter(conf, filePath, keyClass, valueClass);
	
	            } catch (IOException e) {
	                log.debug(e.getMessage());
	                log.info("Failed writing to HDFS. Sleep 15 seconds and retry!");
	                retry = true;
	            } catch (InterruptedException e) {
	                log.debug(e.getMessage());
	                log.info("Thread interrupted. Sleep 15 seconds and retry!");
	            }
	
	        }while (retry);
	        
        }
	
        Text key = new Text();
        BytesWritable val = new BytesWritable();
        long totalBytes = 0L;
        List<String> files = seq.getS3FilesPaths();

        for (String file : files) {
            S3Object s3Object;
            log.info("Sending file "+ file +" from s3 to " + StringUtils.capitalize(mode));
            try {
                s3Object = s3Service.getObject(bucket, file);
                
                if (mode.equals("hadoop")){
	                byte[] bytes = IOUtils.toByteArray(s3Object.getDataInputStream());
	                totalBytes += bytes.length;
	                val.set(bytes, 0, bytes.length);
	                key.set(DigestUtils.sha256Hex(bytes));
	                writer.append(key, val);
	                
                }else if(mode.equals("logstash")){
                	scpS3Object(s3Object);
                }
                
                s3Object.closeDataInputStream();

            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        try {
            if (mode.equals("hadoop")) {writer.close();}
        } catch (IOException e) {
            e.printStackTrace();
        }

        if (mode.equals("hadoop")){
	        log.info("Wrote sequence file [ " + totalBytes + "B ] at: " + seq.getHDFSPath());
	        hdfsURL = seq.getHDFSPath();
        }
    }


    public String getHdfsFile() {
        return hdfsURL;
    }
}

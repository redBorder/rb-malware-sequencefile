package net.redborder.malware.sequencefile.peons;

import net.redborder.malware.sequencefile.SequenceFile;
import net.redborder.malware.sequencefile.util.ConfigFile;
import net.redborder.malware.sequencefile.util.S3HDFSDB;
import net.redborder.malware.sequencefile.util.logger.RbLogger;
import org.apache.commons.codec.digest.DigestUtils;

import com.amazonaws.util.IOUtils;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.jets3t.service.S3Service;
import org.jets3t.service.impl.rest.httpclient.RestS3Service;
import org.jets3t.service.model.S3Object;
import org.jets3t.service.model.StorageObject;
import org.jets3t.service.security.AWSCredentials;
import org.joda.time.DateTime;

import java.io.IOException;
import java.util.List;

import org.apache.log4j.Logger;

/**
 * Created by andresgomez on 3/2/15.
 */
public class SequencePeon extends Thread {
    private Logger log = null;
    S3Service s3Service;
    SequenceFile seq;
    String hdfsURL;
    private S3HDFSDB database;
    private String bucket;


    public SequencePeon(SequenceFile seq) {
        this.seq = seq;
        AWSCredentials awsCredentials = new AWSCredentials((String) ConfigFile.getInstance().getFromGeneral("aws_access"), (String) ConfigFile.getInstance().getFromGeneral("aws_secret"));
        bucket = ConfigFile.getInstance().getFromGeneral("aws_bucket");
        s3Service = new RestS3Service(awsCredentials);
        log = RbLogger.getLogger(SequencePeon.class);
        database = S3HDFSDB.getInstance();
    }

    @Override
    public void run() {

        log.info("Launching sequence peon [" + Thread.currentThread().getId() + "]");
        Configuration conf = new Configuration();
        conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
        conf.addResource("/opt/rb/var/hadoop/etc/hadoop/core-site.xml");
        conf.addResource("/opt/rb/var/hadoop/share/doc/hadoop/hadoop-project-dist/hadoop-common/core-default.xml");


        Path name = new Path(seq.getHDFSPath());
        org.apache.hadoop.io.SequenceFile.Writer writer = null;

        try {
            org.apache.hadoop.io.SequenceFile.Writer.Option filePath = org.apache.hadoop.io.SequenceFile.Writer.file(name);
            org.apache.hadoop.io.SequenceFile.Writer.Option keyClass = org.apache.hadoop.io.SequenceFile.Writer.keyClass(Text.class);
            org.apache.hadoop.io.SequenceFile.Writer.Option valueClass = org.apache.hadoop.io.SequenceFile.Writer.valueClass(BytesWritable.class);
            writer = org.apache.hadoop.io.SequenceFile.createWriter(conf, filePath, keyClass, valueClass);
        } catch (IOException e) {
            e.printStackTrace();
        }

        Text key = new Text();
        BytesWritable val = new BytesWritable();
        long totalBytes = 0L;
        List<String> files = seq.getS3FilesPaths();

        for (String file : files) {
            S3Object s3Object = null;
            log.debug("Getting "+ file +" from s3");
            try {
                s3Object = s3Service.getObject(bucket, file);
                byte[] bytes = IOUtils.toByteArray(s3Object.getDataInputStream());
                totalBytes += bytes.length;
                val.set(bytes, 0, bytes.length);
                key.set(DigestUtils.sha256Hex(bytes));
                writer.append(key, val);
                s3Object.closeDataInputStream();

            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        try {
            writer.close();
        } catch (IOException e) {
            e.printStackTrace();
        }


        log.info("Wrote sequence file [ " + totalBytes + "B ] at: " + seq.getHDFSPath());
        hdfsURL = seq.getHDFSPath();

    }

    public String getHdfsFile() {
        return hdfsURL;
    }
}
